  0%|          | 0/10017 [00:00<?, ?it/s]/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3668: FutureWarning:
`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular
`__call__` method to prepare your inputs and targets.
Here is a short example:
model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)
If you either need to use different keyword arguments for the source and target texts, you should do two calls like
this:
model_inputs = tokenizer(src_texts, ...)
labels = tokenizer(text_target=tgt_texts, ...)
model_inputs["labels"] = labels["input_ids"]
See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.
For a more complete example, see the implementation of `prepare_seq2seq_batch`.
  warnings.warn(formatted_warning, FutureWarning)
/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
  0%|          | 1/10017 [00:01<3:57:13,  1.42s/it]
Traceback (most recent call last):
  File "run.py", line 236, in <module>
    train(model, optimizer, train_loader, dev_loader, epochs=cfg['epochs'])
  File "run.py", line 136, in train
    loss = model(batch[0], batch[1])
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1120, in _call_impl
    result = forward_call(*input, **kwargs)
  File "run.py", line 90, in forward
    outputs = self.bert(input_ids=input_ids, labels=labels)
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/transformers/models/mbart/modeling_mbart.py", line 1370, in forward
    lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/domus/h1/huangjie/miniconda3/envs/py38_apex/lib/python3.8/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 14.61 GiB total capacity; 13.44 GiB already allocated; 131.56 MiB free; 13.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
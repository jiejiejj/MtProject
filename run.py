# -*- coding: utf-8 -*-
"""Glacian-English

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hnw4suDP5CiiACacou2vC10xbYsMfVDy
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.utils.data as Data
from tqdm import tqdm
from sklearn.metrics import *
from transformers import *
from utils import *
import argparse
from datetime import timedelta
import wandb


try:
    from apex import amp
    USE_AMP = True
except ImportError:
    USE_AMP = False
    print('Warning: Apex not installed.')


# transformers.logging.set_verbosity_error()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
eval_pred_list = []

# seeds
def set_seeds(SEED):
    np.random.seed(SEED)
    torch.manual_seed(SEED)

class ParallelCorpus(Data.Dataset):
    def __init__(self, data, cfg, with_labels=True):
        self.data = data
        self.tokenizer = MBart50TokenizerFast.from_pretrained(cfg['model_name'])
        self.body_maxlen = cfg['body_maxlen']
        self.target_maxlen = cfg['target_maxlen']
        self.src_lang = cfg['src_lang']
        self.tgt_lang = cfg['tgt_lang']
        self.with_labels = with_labels  # Is this variable needed?

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        source = self.data[index][0]
        target = self.data[index][1]

        batch = self.tokenizer.prepare_seq2seq_batch(source, tgt_texts=target,
                                                     src_lang=self.src_lang,
                                                     tgt_lang=self.tgt_lang,
                                                     padding='max_length',
                                                     truncation=True,
                                                     max_length=self.body_maxlen,
                                                     max_target_length=self.target_maxlen,
                                                     return_tensors='pt')
        input_ids = batch['input_ids'].squeeze(0)
        target_ids = batch['labels'].squeeze(0)

        # batch = self.tokenizer(source, tgt_texts = target,
        #                         src_lang = self.src_lang,
        #                         tgt_lang = self.tgt_lang,
        #                         padding = 'max_length',
        #                         truncation = True,
        #                         max_length = self.body_maxlen,
        #                         max_target_length = self.target_maxlen,
        #                         return_tensors = 'pt')
        return input_ids, target_ids


class MBart(nn.Module):
    def __init__(self, freeze_bert=False, model_name='facebook/mbart-large-50', hidden_size=768):
        super(MBart, self).__init__()
        self.tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
        self.bert = MBartForConditionalGeneration.from_pretrained(model_name, return_dict=True)
        if freeze_bert:
            for p in self.bert.parameters():
                p.requires_grad = False

    def forward(self, input_ids, labels):
        outputs = self.bert(input_ids=input_ids, labels=labels)
        return outputs.loss

    def generate(self, input_ids, labels, decoder_start_token):
        generated_tokens = self.bert.generate(input_ids, decoder_start_token_id=self.tokenizer.lang_code_to_id[
            decoder_start_token])
        generated_sentence = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        ground_truth_sentence = self.tokenizer.batch_decode(labels, skip_special_tokens=True)[0]
        return generated_sentence, ground_truth_sentence


def train(model, optimizer, train_loader, dev_loader, epochs=1):
    if os.path.exists(cfg['output_model']):
        logger.info('Loading model...')
        checkpoint = torch.load(cfg['output_model'], map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    else:
        model.to(device)
    logger.info('----Training----')

    # wandb
    if cfg['local_rank'] == 0:
        wandb.login(key='...')
        wandb.init(
            project=cfg['task']
        )
        wandb.watch(model, log='all', log_freq=100)

    # apex
    if USE_AMP:
        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')

    # model
    model = torch.nn.parallel.DistributedDataParallel(
        model,
        device_ids=[cfg['local_rank']],
        output_device=cfg['local_rank']
    )

    for epoch in range(epochs):
        model.train()
        logger.info('Epoch', epoch)
        for i, batch in enumerate(tqdm(train_loader)):
            batch = tuple(t.to(device) for t in batch)
            loss = model(batch[0], batch[1])
            # loss = model(batch)
            logger.info(i, loss.item())
            optimizer.zero_grad()
            # apex
            if USE_AMP:
                with amp.scale_loss(loss, optimizer) as scaled_loss:
                    scaled_loss.backward()
            else:
                loss.backward()

            optimizer.step()

            if i and i % 2000 == 0 and cfg['local_rank'] == 0:
                eval(i, model, dev_loader)
            torch.distributed.barrier()

    if cfg['local_rank'] == 0:
        wandb.finish()


def eval(cur_step, model, val_dataloader):
    logger.info('Eval starts for current step {}'.format(cur_step))
    device = torch.device("cuda", cfg['local_rank']) if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)
    model.eval()
    eval_loss, eval_accuracy, eval_steps, eval_recall, eval_fl = 0, 0, 0, 0, 0
    with torch.no_grad():
        for idx, batch in enumerate(tqdm(val_dataloader)):
            eval_steps += 1
            batch = tuple(t.to(device) for t in batch)
            if idx == 0:
                generated_sentence, ground_truth_sentence = model.generate(batch[0], batch[1], 'en_XX')
                logger.info('Prediction Sentence:', generated_sentence)
                logger.info('Ground Truth Sentence:', ground_truth_sentence)
            loss = model(batch[0], batch[1])
            eval_loss += loss.item()
    model.train()
    global best_loss
    if eval_loss / eval_steps < best_loss:
        best_loss = eval_loss / eval_steps
        logger.info('Congrats!')
        # save(model, optimizer)
        # get_ckpt_path(cfg).format(step)

    logger.info('step {}, validation loss: {}'.format(cur_step, eval_loss / eval_steps))


def process_data(file_gl, file_en):
    data = []
    with open(get_abs_path(file_gl)) as f:
        for line in f.readlines():
            data.append([line.strip('\n')])
    with open(get_abs_path(file_en)) as f:
        for i, line in enumerate(f.readlines()):
            data[i].append(line.strip('\n'))
    logger.info("Read {} pieces of samples".format(len(data)))
    return data


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='News Headline Generation')
    parser.add_argument('-c', '--config', default='config.json', type=str, required=False,
                        help='config file path')
    parser.add_argument('-t', '--type', default='predict', type=str, required=False, help='train / eval / predict')
    args, unknown = parser.parse_known_args()

    cfg = load_json(get_abs_path('config.json'))
    cfg['time_str'] = args.type + '_' + get_datetime()
    set_seeds(cfg['seeds'])

    # gpu
    os.environ["CUDA_VISIBLE_DEVICES"] = cfg['gpu']

    # dpp
    if args.type == 'train':
        torch.distributed.init_process_group(backend="nccl", timeout=timedelta(hours=4))
        local_rank = torch.distributed.get_rank()
        torch.cuda.set_device(local_rank)
        cfg['local_rank'] = local_rank
        torch.cuda.synchronize()

    # logger
    logger = get_logger(cfg)

    # data
    train_data = process_data(cfg['train_src_path'], cfg['train_tgt_path'])
    dev_data = process_data(cfg['dev_src_path'], cfg['dev_tgt_path'])

    print('Reading training data...')
    train_set = ParallelCorpus(train_data, cfg)
    train_loader = Data.DataLoader(train_set, batch_size=cfg['batch_size'], shuffle=True)

    print('Reading development data...')
    dev_set = ParallelCorpus(dev_data, cfg)
    dev_loader = Data.DataLoader(dev_set, batch_size=cfg['batch_size'], shuffle=True)

    model = MBart(freeze_bert=False, model_name=cfg['model_name'], hidden_size=cfg['hidden_size'])
    optimizer = AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])

    train(model, optimizer, train_loader, dev_loader, epochs=cfg['epochs'])